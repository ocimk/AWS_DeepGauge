{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BYOM: Train with Custom Training Script, Compile with Neo, and Deploy on SageMaker\n",
    "\n",
    "This notebook can be compared to [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) in terms of its functionality. We will do the same classification task, but this time we will compile the trained model using the Neo API backend, to optimize for our choice of hardware. Finally, we setup a real-time hosted endpoint in SageMaker for our compiled model using the Neo Deep Learning Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "# type(data_sets.train.images)\n",
    "# data_sets.train.num_examples\n",
    "# data_sets.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "# import tensorflow as tf\n",
    "\n",
    "# data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "# utils.convert_to(data_sets.train, 'train', 'data')\n",
    "# utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "# utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training \n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import tensorflow as tf\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
      "\n",
      "INPUT_TENSOR_NAME = 'inputs'\n",
      "SIGNATURE_NAME = 'predictions'\n",
      "\n",
      "LEARNING_RATE = 0.001\n",
      "\n",
      "\n",
      "def model_fn(features, labels, mode, params):\n",
      "    # Input Layer\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n",
      "\n",
      "    # Convolutional Layer #1\n",
      "    conv1 = tf.layers.conv2d(\n",
      "        inputs=input_layer,\n",
      "        filters=32,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "\n",
      "    # Pooling Layer #1\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\n",
      "    conv2 = tf.layers.conv2d(\n",
      "        inputs=pool1,\n",
      "        filters=64,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Dense Layer\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
      "    dropout = tf.layers.dropout(\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n",
      "\n",
      "    # Logits Layer\n",
      "    logits = tf.layers.dense(inputs=dropout, units=31)\n",
      "\n",
      "    # Define operations\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
      "\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\n",
      "        global_step = tf.train.get_or_create_global_step()\n",
      "        label_indices = tf.cast(labels, tf.int32)\n",
      "        loss = tf.losses.softmax_cross_entropy(\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=31), logits=logits)\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\n",
      "\n",
      "    if mode == Modes.PREDICT:\n",
      "        predictions = {\n",
      "            'classes': predicted_indices,\n",
      "            'probabilities': probabilities\n",
      "        }\n",
      "        export_outputs = {\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\n",
      "\n",
      "    if mode == Modes.TRAIN:\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
      "\n",
      "    if mode == Modes.EVAL:\n",
      "        eval_metric_ops = {\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
      "\n",
      "\n",
      "def serving_input_fn(params):\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784* 1])}\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
      "\n",
      "\n",
      "def read_and_decode(filename_queue):\n",
      "    reader = tf.TFRecordReader()\n",
      "    _, serialized_example = reader.read(filename_queue)\n",
      "\n",
      "    features = tf.parse_single_example(\n",
      "        serialized_example,\n",
      "        features={\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\n",
      "        })\n",
      "\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
      "    image.set_shape([784* 1])\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\n",
      "    label = tf.cast(features['label'], tf.int32)\n",
      "\n",
      "    return image, label\n",
      "\n",
      "\n",
      "def train_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def eval_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\n",
      "    test_file = os.path.join(training_dir, training_filename)\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\n",
      "\n",
      "    image, label = read_and_decode(filename_queue)\n",
      "    images, labels = tf.train.batch(\n",
      "        [image, label], batch_size=batch_size,\n",
      "        capacity=1000 + 3 * batch_size)\n",
      "\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\n",
      "\n",
      "\n",
      "def neo_preprocess(payload, content_type):\n",
      "    import logging\n",
      "    import numpy as np\n",
      "    import io\n",
      "\n",
      "    logging.info('Invoking user-defined pre-processing function')\n",
      "\n",
      "    if content_type != 'application/x-image' and content_type != 'application/vnd+python.numpy+binary':\n",
      "        raise RuntimeError('Content type must be application/x-image or application/vnd+python.numpy+binary')\n",
      "\n",
      "    f = io.BytesIO(payload)\n",
      "    image = np.load(f) * 255\n",
      "\n",
      "    return image\n",
      "\n",
      "\n",
      "### NOTE: this function cannot use MXNet\n",
      "def neo_postprocess(result):\n",
      "    import logging\n",
      "    import numpy as np\n",
      "    import json\n",
      "\n",
      "    logging.info('Invoking user-defined post-processing function')\n",
      "\n",
      "    # Softmax (assumes batch size 1)\n",
      "    result = np.squeeze(result)\n",
      "    result_exp = np.exp(result - np.max(result))\n",
      "    result = result_exp / np.sum(result_exp)\n",
      "\n",
      "    response_body = json.dumps(result.tolist())\n",
      "    content_type = 'application/json'\n",
      "\n",
      "    return response_body, content_type"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script here is and adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist). It provides a ```model_fn(features, labels, mode)```, which is used for training, evaluation and inference. See [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) for more details about the training script.\n",
    "\n",
    "At the end of the training script, there are two additional functions, to be used with Neo Deep Learning Runtime:\n",
    "* `neo_preprocess(payload, content_type)`: Function that takes in the payload and Content-Type of each incoming request and returns a NumPy array\n",
    "* `neo_postprocess(result)`: Function that takes the prediction results produced by Deep Learining Runtime and returns the response body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TENSOR_NAME = 'inputs'\n",
    "SIGNATURE_NAME = 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow py2 container will be deprecated soon.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-29 20:44:24 Starting - Starting the training job...\n",
      "2019-08-29 20:44:25 Starting - Launching requested ML instances...\n",
      "2019-08-29 20:45:24 Starting - Preparing the instances for training......\n",
      "2019-08-29 20:46:19 Downloading - Downloading input data\n",
      "2019-08-29 20:46:19 Training - Downloading the training image....\n",
      "\u001b[31m2019-08-29 20:46:55,879 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:55,880 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:55,902 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,308 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,309 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,310 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fad28a03f10>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[31mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[31mallow_soft_placement: true\u001b[0m\n",
      "\u001b[31mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints', '_master': ''}\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,311 INFO - tensorflow - Skip starting Tensorflow server as there is only one node in the cluster.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.394260: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.394775: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,535 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,537 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,559 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,878 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58,879 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.887755: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.887792: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.902449: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.902482: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.922064: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:58.922129: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:46:59,228 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:00.850435: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:00.850495: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01,169 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01,174 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01,199 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01.272650: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01.272701: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:01,810 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:04.133007: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:04.133057: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\n",
      "2019-08-29 20:46:55 Training - Training image download completed. Training in progress.\u001b[31m2019-08-29 20:47:13,604 INFO - tensorflow - loss = 3.4388936, step = 0\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:15,019 INFO - tensorflow - global_step/sec: 70.6711\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:15,020 INFO - tensorflow - loss = 1.3588327, step = 100 (1.416 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:16,565 INFO - tensorflow - global_step/sec: 64.6676\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:16,566 INFO - tensorflow - loss = 0.57172257, step = 200 (1.546 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:18,095 INFO - tensorflow - global_step/sec: 65.3417\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:18,097 INFO - tensorflow - loss = 0.44323862, step = 300 (1.531 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:19,619 INFO - tensorflow - global_step/sec: 65.6319\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:19,620 INFO - tensorflow - loss = 0.18154915, step = 400 (1.523 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:21,145 INFO - tensorflow - global_step/sec: 65.5222\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:21,146 INFO - tensorflow - loss = 0.068072975, step = 500 (1.526 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:22,704 INFO - tensorflow - global_step/sec: 64.1494\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:22,705 INFO - tensorflow - loss = 0.07073947, step = 600 (1.559 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:24,234 INFO - tensorflow - global_step/sec: 65.3723\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:24,235 INFO - tensorflow - loss = 0.117753685, step = 700 (1.530 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:25,760 INFO - tensorflow - global_step/sec: 65.5102\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:25,761 INFO - tensorflow - loss = 0.10921387, step = 800 (1.527 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:27,307 INFO - tensorflow - global_step/sec: 64.6395\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:27,308 INFO - tensorflow - loss = 0.069906466, step = 900 (1.547 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:28,833 INFO - tensorflow - global_step/sec: 65.5578\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:28,834 INFO - tensorflow - loss = 0.03677451, step = 1000 (1.526 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:30,357 INFO - tensorflow - global_step/sec: 65.6056\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:30,358 INFO - tensorflow - loss = 0.023792312, step = 1100 (1.524 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:31,916 INFO - tensorflow - global_step/sec: 64.161\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:31,916 INFO - tensorflow - loss = 0.025906855, step = 1200 (1.558 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:33,448 INFO - tensorflow - global_step/sec: 65.232\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:33,450 INFO - tensorflow - loss = 0.028308067, step = 1300 (1.533 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:34,967 INFO - tensorflow - global_step/sec: 65.8571\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:34,968 INFO - tensorflow - loss = 0.02359196, step = 1400 (1.518 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:36,490 INFO - tensorflow - global_step/sec: 65.6644\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:36,491 INFO - tensorflow - loss = 0.004253212, step = 1500 (1.523 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:38,041 INFO - tensorflow - global_step/sec: 64.4644\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:38,042 INFO - tensorflow - loss = 0.004232692, step = 1600 (1.551 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:39,567 INFO - tensorflow - global_step/sec: 65.5402\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:39,568 INFO - tensorflow - loss = 0.019456638, step = 1700 (1.526 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:41,086 INFO - tensorflow - global_step/sec: 65.8203\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:41,087 INFO - tensorflow - loss = 0.009552206, step = 1800 (1.519 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:42,623 INFO - tensorflow - global_step/sec: 65.0661\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:42,624 INFO - tensorflow - loss = 0.00386671, step = 1900 (1.537 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:44,156 INFO - tensorflow - global_step/sec: 65.25\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:44,157 INFO - tensorflow - loss = 0.002595074, step = 2000 (1.532 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:45,675 INFO - tensorflow - global_step/sec: 65.8039\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:45,676 INFO - tensorflow - loss = 0.0014016512, step = 2100 (1.520 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:47,213 INFO - tensorflow - global_step/sec: 65.0478\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:47,214 INFO - tensorflow - loss = 0.003995079, step = 2200 (1.537 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:48,738 INFO - tensorflow - global_step/sec: 65.5548\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:48,739 INFO - tensorflow - loss = 0.0055622254, step = 2300 (1.526 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:50,267 INFO - tensorflow - global_step/sec: 65.4228\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:50,268 INFO - tensorflow - loss = 0.0042504575, step = 2400 (1.528 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:51,804 INFO - tensorflow - global_step/sec: 65.0431\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:51,805 INFO - tensorflow - loss = 0.010185258, step = 2500 (1.538 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:53,341 INFO - tensorflow - global_step/sec: 65.045\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:53,342 INFO - tensorflow - loss = 0.0045113773, step = 2600 (1.537 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:54,865 INFO - tensorflow - global_step/sec: 65.6552\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:54,866 INFO - tensorflow - loss = 0.0012648698, step = 2700 (1.523 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:56,388 INFO - tensorflow - global_step/sec: 65.6339\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:56,389 INFO - tensorflow - loss = 0.0010585771, step = 2800 (1.523 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:57,926 INFO - tensorflow - global_step/sec: 65.0096\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:57,927 INFO - tensorflow - loss = 0.003384316, step = 2900 (1.538 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:59,446 INFO - tensorflow - global_step/sec: 65.8256\u001b[0m\n",
      "\u001b[31m2019-08-29 20:47:59,447 INFO - tensorflow - loss = 0.0006672621, step = 3000 (1.519 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:00,985 INFO - tensorflow - global_step/sec: 64.9785\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:00,986 INFO - tensorflow - loss = 0.0003540837, step = 3100 (1.539 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:02,526 INFO - tensorflow - global_step/sec: 64.8807\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:02,527 INFO - tensorflow - loss = 0.0005963914, step = 3200 (1.541 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:04,050 INFO - tensorflow - global_step/sec: 65.6131\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:04,051 INFO - tensorflow - loss = 0.024802519, step = 3300 (1.524 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:05,576 INFO - tensorflow - global_step/sec: 65.5156\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:05,577 INFO - tensorflow - loss = 0.0347445, step = 3400 (1.526 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:07,119 INFO - tensorflow - global_step/sec: 64.8404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:07,120 INFO - tensorflow - loss = 0.029802075, step = 3500 (1.542 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:08,649 INFO - tensorflow - global_step/sec: 65.335\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:08,650 INFO - tensorflow - loss = 0.007366611, step = 3600 (1.530 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:10,174 INFO - tensorflow - global_step/sec: 65.5926\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:10,175 INFO - tensorflow - loss = 0.0020345643, step = 3700 (1.525 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:11,698 INFO - tensorflow - global_step/sec: 65.6104\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:11,699 INFO - tensorflow - loss = 0.00028207246, step = 3800 (1.524 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:13,242 INFO - tensorflow - global_step/sec: 64.748\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:13,243 INFO - tensorflow - loss = 0.00214662, step = 3900 (1.544 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:14,767 INFO - tensorflow - global_step/sec: 65.6027\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:14,768 INFO - tensorflow - loss = 0.012092317, step = 4000 (1.524 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:16,291 INFO - tensorflow - global_step/sec: 65.6081\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:16,292 INFO - tensorflow - loss = 0.00072588085, step = 4100 (1.524 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:17,823 INFO - tensorflow - global_step/sec: 65.2553\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:17,824 INFO - tensorflow - loss = 0.0005845817, step = 4200 (1.532 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:19,354 INFO - tensorflow - global_step/sec: 65.3419\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:19,355 INFO - tensorflow - loss = 5.050747e-05, step = 4300 (1.530 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:20,885 INFO - tensorflow - global_step/sec: 65.3051\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:20,886 INFO - tensorflow - loss = 0.00043035357, step = 4400 (1.531 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:22,425 INFO - tensorflow - global_step/sec: 64.9476\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:22,426 INFO - tensorflow - loss = 0.0023587185, step = 4500 (1.540 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:23,954 INFO - tensorflow - global_step/sec: 65.3894\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:23,955 INFO - tensorflow - loss = 0.00088492647, step = 4600 (1.529 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:25,486 INFO - tensorflow - global_step/sec: 65.2793\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:25,487 INFO - tensorflow - loss = 0.00020692471, step = 4700 (1.532 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:27,025 INFO - tensorflow - global_step/sec: 64.9611\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:27,026 INFO - tensorflow - loss = 0.00014707209, step = 4800 (1.539 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:28,554 INFO - tensorflow - global_step/sec: 65.4273\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:28,555 INFO - tensorflow - loss = 0.0038101932, step = 4900 (1.528 sec)\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:30,069 INFO - tensorflow - Saving checkpoints for 5000 into s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32.215516: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32.215596: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32,531 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32,746 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32,769 INFO - tensorflow - Starting evaluation at 2019-08-29-20:48:32\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32,854 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:32,907 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/model.ckpt-5000\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,197 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,205 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,411 INFO - tensorflow - Evaluation [10/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,562 INFO - tensorflow - Evaluation [20/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,716 INFO - tensorflow - Evaluation [30/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:33,866 INFO - tensorflow - Evaluation [40/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,016 INFO - tensorflow - Evaluation [50/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,170 INFO - tensorflow - Evaluation [60/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,319 INFO - tensorflow - Evaluation [70/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,468 INFO - tensorflow - Evaluation [80/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,619 INFO - tensorflow - Evaluation [90/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,770 INFO - tensorflow - Evaluation [100/100]\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,799 INFO - tensorflow - Finished evaluation at 2019-08-29-20:48:34\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34,799 INFO - tensorflow - Saving dict for global step 5000: accuracy = 0.9303, global_step = 5000, loss = 0.3389176\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.806727: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.806800: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.820290: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.820357: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.835478: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:34.835513: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,047 INFO - tensorflow - Saving 'checkpoint_path' summary for global step 5000: s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/model.ckpt-5000\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.134280: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.134347: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.152552: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.152587: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.163473: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.163507: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.180694: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.180762: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.195439: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.195473: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.208262: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.208344: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,297 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,366 INFO - tensorflow - Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,447 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/model.ckpt-5000\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,784 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1018: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,785 INFO - tensorflow - Assets added to graph.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35,785 INFO - tensorflow - No assets to write.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.792883: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.792920: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.809638: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.809674: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.825458: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:35.825525: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:36.989225: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:36.989266: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37,017 INFO - tensorflow - SavedModel written to: s3://sagemaker-us-east-2-185004506247/sagemaker-tensorflow-2019-08-29-20-44-23-837/checkpoints/export/Servo/temp-1567111715/saved_model.pb\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.023809: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.023849: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.472728: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.472807: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.492186: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37.492263: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37,582 INFO - tensorflow - Loss for final step: 0.0022287217.\u001b[0m\n",
      "\u001b[31m2019-08-29 20:48:37,835 INFO - tf_container - Downloaded saved model at /opt/ml/model/export/Servo/1567111715\u001b[0m\n",
      "\n",
      "2019-08-29 20:48:47 Uploading - Uploading generated training model\n",
      "2019-08-29 20:48:47 Completed - Training job completed\n",
      "Training seconds: 160\n",
      "Billable seconds: 160\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.11.0',\n",
    "                             training_steps=5000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.p3.2xlarge')\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in two **ml.c4.xlarge** instances. The logs above will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for TF serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions (the old way)\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "label is 27\n",
      "prediction is 27\n",
      "========================================\n",
      "label is 28\n",
      "prediction is 28\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 12\n",
      "prediction is 12\n",
      "========================================\n",
      "label is 22\n",
      "prediction is 22\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 30\n",
      "prediction is 30\n",
      "========================================\n",
      "label is 21\n",
      "prediction is 20\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 25\n",
      "prediction is 25\n",
      "========================================\n",
      "label is 15\n",
      "prediction is 15\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 19\n",
      "prediction is 19\n",
      "========================================\n",
      "label is 13\n",
      "prediction is 13\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 30\n",
      "prediction is 30\n",
      "========================================\n",
      "label is 30\n",
      "prediction is 30\n",
      "========================================\n",
      "label is 12\n",
      "prediction is 11\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 11\n",
      "prediction is 12\n",
      "========================================\n",
      "label is 22\n",
      "prediction is 22\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 10\n",
      "prediction is 10\n",
      "========================================\n",
      "label is 23\n",
      "prediction is 23\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 25\n",
      "prediction is 25\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 21\n",
      "prediction is 21\n",
      "========================================\n",
      "label is 23\n",
      "prediction is 23\n",
      "========================================\n",
      "label is 27\n",
      "prediction is 27\n",
      "========================================\n",
      "label is 29\n",
      "prediction is 29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "with open('./data/test_dataset.pickle', 'rb') as handle:\n",
    "    test_dataset = pickle.load(handle)\n",
    "\n",
    "    \n",
    "# len(test_dataset['images'][0])\n",
    "for i in range(32):\n",
    "    data = test_dataset['images'][i]\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[len(data), len(data)], dtype=tf.float32)\n",
    "    tensor_proto\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    label = test_dataset['labels'][i]\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64_val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))\n",
    "    # tensor_proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model using Neo\n",
    "\n",
    "Now the model is ready to be compiled by Neo to be optimized for our hardware of choice. We are using the  ``TensorFlowEstimator.compile_model`` method to do this. For this example, our target hardware is ``'ml_c5'``. You can changed these to other supported target hardware if you prefer.\n",
    "\n",
    "## Compiling the model\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?.....!"
     ]
    }
   ],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "optimized_estimator = mnist_estimator.compile_model(target_instance_family='ml_c5', \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.11.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.tensorflow.model.TensorFlowModel at 0x7f31bbf3eba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/vnd+python.numpy+binary'\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.reshape((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = test_dataset['images'][i]\n",
    "    \n",
    "    # Display image\n",
    "    im = PIL.Image.fromarray(data.reshape((28,28))).convert('L')\n",
    "    display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = test_dataset['labels'][i]\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(optimized_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
